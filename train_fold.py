import torch
import torch.optim as optim
import numpy as np
import scipy.stats as stats # ç”¨æ–¼è¨ˆç®—ä¿¡è³´å€é–“
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import StratifiedKFold
from torch.utils.data import DataLoader, SubsetRandomSampler

# ä¿æŒä½ åŸæœ¬çš„å¼•ç”¨
from dataset import get_dataloaders, CalligraphyDataset,get_full_dataset # å‡è¨­ä½  dataset.py è£¡æœ‰é€™å€‹é¡åˆ¥
from core.model import MultiTaskCNN, MultiTaskLoss
from core.trainer import train_one_epoch, validate
from core.visualize import plot_history
from torch.optim.lr_scheduler import ReduceLROnPlateau
from utils.utils import EarlyStopping 
import random
import os

# --- è¨­å®šåƒæ•¸ ---
DATA_ROOT = 'data'
CSV_PATH = 'logs/Summary.csv'
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EPOCHS = 50
K_FOLDS = 5 # æ–°å¢ï¼šè¨­å®š 5 æŠ˜
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"éš¨æ©Ÿç¨®å­å·²é–å®šç‚º: {seed}")

def calculate_confidence_interval(data, confidence=0.95):
    """è¨ˆç®—å¹³å‡å€¼ã€è®Šç•°æ•¸èˆ‡ 95% ä¿¡è³´å€é–“"""
    n = len(data)
    mean = np.mean(data)
    var = np.var(data, ddof=1) # æ¨£æœ¬è®Šç•°æ•¸
    std_err = stats.sem(data)
    h = std_err * stats.t.ppf((1 + confidence) / 2., n - 1)
    return mean, var, (mean - h, mean + h)

def main():
    set_seed(42)
    
    # 1. æº–å‚™å®Œæ•´è³‡æ–™é›† (ç§»é™¤åŸæœ¬é‡è¤‡çš„ full_dataset å®šç¾©)
    # ç›´æ¥ä½¿ç”¨å„ªåŒ–éçš„ get_full_dataset å–å¾—å…©å€‹ Dataset å°è±¡
    train_ds, val_ds, num_authors, num_styles, all_auth_lbls, all_style_lbls = get_full_dataset(DATA_ROOT, CSV_PATH)
    
    # 2. åˆå§‹åŒ– 5-Fold åˆ†å‰²å™¨
    skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)
    
    # å­˜æ”¾çµæœçš„å®¹å™¨
    fold_auth_accs = []
    fold_style_accs = []
    best_overall_avg_acc = 0.0
    best_fold_idx = -1

    print(f"\né–‹å§‹åŸ·è¡Œ {K_FOLDS}-Fold äº¤å‰é©—è­‰ (è³‡æ–™ç¸½æ•¸: {len(train_ds)})")

    # 3. é€²å…¥ 5-Fold è¿´åœˆ
    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_auth_lbls)), all_auth_lbls)):
        print(f"\n" + "="*30)
        print(f"FOLD {fold + 1} / {K_FOLDS}")
        print("="*30)

        # 3.1 å»ºç«‹ç•¶å‰ Fold çš„ DataLoader (ç¢ºä¿è¨“ç·´æœ‰å¢å¼·ï¼Œé©—è­‰ç„¡å¢å¼·)
        train_sampler = SubsetRandomSampler(train_idx)
        val_sampler = SubsetRandomSampler(val_idx)
        
        # é€™è£¡çš„ num_workers å»ºè­°æ ¹æ“šä½ çš„ CPU æ ¸å¿ƒè¨­å®š (ä¾‹å¦‚ 2 æˆ– 4) ä»¥åŠ é€Ÿè®€å–
        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)
        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=0)

        # 3.2 é‡æ–°è¨ˆç®—æ¬Šé‡ (ä½¿ç”¨å·²ç¶“æå–å¥½çš„ all_auth_lbls)
        curr_train_auth = all_auth_lbls[train_idx]
        curr_train_style = all_style_lbls[train_idx]
        
        auth_weights = torch.tensor(compute_class_weight('balanced', classes=np.unique(curr_train_auth), y=curr_train_auth), dtype=torch.float)
        style_weights = torch.tensor(compute_class_weight('balanced', classes=np.unique(curr_train_style), y=curr_train_style), dtype=torch.float)

        # 3.3 åˆå§‹åŒ–æ¨¡å‹èˆ‡æå¤±å‡½æ•¸
        model = MultiTaskCNN(num_authors, num_styles).to(DEVICE)
        criterion = MultiTaskLoss(author_weights=auth_weights, style_weights=style_weights, device=DEVICE)
        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)
        
        fold_model_path = f'weights/best_model_fold_{fold+1}.pth'
        # å»ºè­° EarlyStopping çš„ path ä¹Ÿè¦éš¨ fold æ”¹è®Š
        early_stopping = EarlyStopping(patience=8, verbose=True, path=fold_model_path)

        # 3.4 è¨“ç·´ Fold
        best_fold_auth_acc = 0.0
        best_fold_style_acc = 0.0

        for epoch in range(EPOCHS):
            # ä½¿ç”¨æˆ‘å€‘å®šç¾©å¥½çš„ train_loader å’Œ val_loader
            t_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)
            v_loss, acc_author, acc_style = validate(model, val_loader, criterion, DEVICE)
            
            scheduler.step(v_loss)
            early_stopping(v_loss, model)

            if acc_author > best_fold_auth_acc: best_fold_auth_acc = acc_author
            if acc_style > best_fold_style_acc: best_fold_style_acc = acc_style

            if early_stopping.early_stop: break

        fold_auth_accs.append(best_fold_auth_acc)
        fold_style_accs.append(best_fold_style_acc)
        
        current_avg = (best_fold_auth_acc + best_fold_style_acc) / 2
        if current_avg > best_overall_avg_acc:
            best_overall_avg_acc = current_avg
            best_fold_idx = fold + 1
            # å„²å­˜å…¨åŸŸæœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'weights/best_model.pth')

    # 4. çµ±è¨ˆé‹ç®—
    a_mean, a_var, a_ci = calculate_confidence_interval(fold_auth_accs)
    s_mean, s_var, s_ci = calculate_confidence_interval(fold_style_accs)

    # 5. è¼¸å‡ºå ±å‘Š
    print("\n" + "â˜…"*40)
    print(f"ğŸ‰ 5-Fold äº¤å‰é©—è­‰æœ€çµ‚çµ±è¨ˆå ±å‘Š")
    print("â˜…"*40)
    print(f"æœ€ä½³ Fold æ¨¡å‹: ç¬¬ {best_fold_idx} çµ„ (å·²å­˜ç‚º weights/best_model.pth)")
    print("-" * 20)
    print(f"[ä½œè€…è¾¨è­˜ Author Accuracy]")
    print(f"  å¹³å‡å€¼: {a_mean:.2f}%")
    print(f"  è®Šç•°æ•¸: {a_var:.4f}")
    print(f"  95% ä¿¡è³´å€é–“: [{a_ci[0]:.2f}%, {a_ci[1]:.2f}%]")
    print("-" * 20)
    print(f"[æ›¸é«”è¾¨è­˜ Style Accuracy]")
    print(f"  å¹³å‡å€¼: {s_mean:.2f}%")
    print(f"  è®Šç•°æ•¸: {s_var:.4f}")
    print(f"  95% ä¿¡è³´å€é–“: [{s_ci[0]:.2f}%, {s_ci[1]:.2f}%]")
    print("â˜…"*40)

if __name__ == '__main__':
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs('weights', exist_ok=True)
    main()